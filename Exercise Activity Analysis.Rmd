---
title: "Exercise Activity Analysis"
output:
  html_document:
    keep_md: yes
---


### Synopsis  

In this report we analyze data from a group of 6 people who exercised and measured their activity using accelerometers on the belt, forearm, arm, and dumbell. A prediction model is built using this data to predict the manner in which the partcipants exercised.

### Data Processing

We downloaded and analyzed the training and test data sets. We used the 
repeated k-fold Cross Validation process with k=10 and number of repeats=3.
For preprocessing, we used "knnimpute" to impute missing values and standardize.
Then we applied classification tree, Naive Bayes  and Random Forest algorithms. 


```{r, cache=TRUE,warning=FALSE}
library(caret)
set.seed(32343)
training <- read.csv("pml-training.csv")
dim(training)
testing <- read.csv("pml-testing.csv")
dim(testing)
summary(training$classe)

#Remove unwanted variables
training1 <- subset(training, select = -c(X,user_name,raw_timestamp_part_1,raw_timestamp_part_2,cvtd_timestamp))
testing1 <- subset(testing, select = -c(X,user_name,raw_timestamp_part_1,raw_timestamp_part_2,cvtd_timestamp))
#Convert to numeric except first (new_window) and last (classe)
training1[, 2:154] <- sapply(training1[, 2:154], as.numeric)
testing1[, 2:154] <- sapply(testing1[, 2:154], as.numeric)

#impute NAs to column means
library(plyr)
impute.mean <- function(x) replace(x, is.na(x), mean(x, na.rm = TRUE))
training2 <- sapply(training1[,2:154], function(x){impute.mean(x)})
training2 <- data.frame(training2)
training2 <- cbind(new_window=training1$new_window,training2)
training2 <- cbind(training2,classe=training1$classe)

for (i in 2:154)
{
      testing1[,i]<- replace(testing1[,i], is.na(testing1[,i]), 
                             mean(training2[,i])) 
}

#remove highly correlated variables
descrCorr <- cor(training2[,2:154])
highCorr <- findCorrelation(descrCorr, 0.90)
training2 <- training2[, -highCorr]
testing2 <- testing1[, -highCorr]


train_control <- trainControl(method="repeatedcv", number=10, repeats=3)

start.time <- Sys.time()

# modelrpart <- train(classe~., data=training2, trControl=train_control, preProcess="knnImpute",method="rpart")
# modelnb <- train(classe~., data=training2, trControl=train_control, preProcess="knnImpute",method="nb")
modelrf <- train(classe~., data=training2, trControl=train_control,method="rf")
      
#confusionMatrix(predict(modelrpart, training2[,1:154]), training2$classe)
#confusionMatrix(predict(modelnb, training2[,1:154]), training2$classe)
confusionMatrix(predict(modelrf, training2[,1:154]), training2$classe)

end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken
time.taken

```



### Results

TO estimate out of sample error, we looked accuracy, sensitivity, specificity, positive predictive value and negative predictive value in the confusion matrix generated by different algorithms. Random Forest produced the best results.
See below for the most important variables. A plot has been created with the 
top two variables in importance and we see distinct groupings.


```{r}
varImp(modelrf)

qplot(min_roll_forearm,var_accel_dumbbell,colour=classe,data=training2)

```
